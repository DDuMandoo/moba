import os
import re
import csv
import time
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoSuchElementException
from multiprocessing import Process

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

chromedriver_path = r"C:\Users\SSAFY\Downloads\chromedriver.exe"
output_file = "merged_places.csv"
progress_log = "progress.log"

seoul_gu_list = [
    'ë§ˆí¬êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬',
    'ë™ëŒ€ë¬¸êµ¬', 'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬', 'ë…¸ì›êµ¬', 'ì¤‘ë‘êµ¬', 'ê°•ë™êµ¬', 'ì†¡íŒŒêµ¬',
    'ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ê´€ì•…êµ¬', 'ë™ì‘êµ¬', 'ì˜ë“±í¬êµ¬', 'ê¸ˆì²œêµ¬', 'êµ¬ë¡œêµ¬', 'ì–‘ì²œêµ¬', 'ê°•ì„œêµ¬'
]
search_list = [
    # ìŒì‹
    "í•œì‹", "ì–‘ì‹", "ì¤‘ì‹", "ì¼ì‹", "ê³ ê¸°", "íšŒ", "êµ­ë°¥", "ì„¤ë íƒ•", "ë¶„ì‹", "ì¡±ë°œ", "ë³´ìŒˆ", "ì¹˜í‚¨", "ë·”í˜", "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ", "ì•„ì‹œì•„ìŒì‹",
    
    # ì¹´í˜/ë””ì €íŠ¸
    "ê°ì„±ì¹´í˜", "ë””ì €íŠ¸", "ë£¨í”„íƒ‘ ì¹´í˜", "ë¸ŒëŸ°ì¹˜ ì¹´í˜", "ì¹´ê³µ", "ì¼€ì´í¬", "ë¹µ", "ì•„ì´ìŠ¤í¬ë¦¼", "ë¹™ìˆ˜", "ë² ì´ì»¤ë¦¬", "í”„ë Œì°¨ì´ì¦ˆ ì¹´í˜", "ì „í†µì°»ì§‘",

    # ë¬¸í™”/ì—¬ê°€
    "ì „ì‹œê´€", "ë¯¸ìˆ ê´€", "ê³µì—°ì¥", "ë¬¸í™”ì„¼í„°", "ì„œì ", "ë³´ë“œì¹´í˜", "pcë°©", "ê³µë°©", "ì˜í™”ê´€", "ë…¸ë˜ë°©", "ë‹¹êµ¬ì¥", "ìŒì•…",

    # ìˆ ì§‘
    "ì´ìì¹´ì•¼", "ì™€ì¸ë°”", "ì¹µí…Œì¼ë°”", "í˜¸í”„", "ìˆ ì§‘", "ì•ˆì£¼", "í¬ì¥ë§ˆì°¨",

    # ìš´ë™
    "í—¬ìŠ¤ì¥", "í•„ë¼í…ŒìŠ¤", "ìš”ê°€", "í´ë¼ì´ë°", "ë³¼ë§", "íƒêµ¬", "ìˆ˜ì˜", "ë†êµ¬", "ìŠ¤ì¿¼ì‹œ", "ì¶•êµ¬", "ì•¼êµ¬", "ìŠ¤í¬ì¸ "
]

options = webdriver.ChromeOptions()
options.add_argument("lang=ko_KR")
options.add_argument("user-agent=Mozilla/5.0")

def get_lat_lng(place_id):
    url = f"https://place.map.kakao.com/{place_id}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=3)
        soup = BeautifulSoup(res.text, "html.parser")
        og_image = soup.find("meta", {"property": "og:image"})
        if og_image:
            content = og_image.get("content", "")
            match = re.search(r"m=([0-9.]+)%2C([0-9.]+)", content)
            if match:
                lng, lat = match.group(1), match.group(2)
                return lat, lng
    except:
        pass
    return "", ""

def crawl_keyword_gu(keyword, gu):
    service = Service(executable_path=chromedriver_path)
    driver = webdriver.Chrome(service=service, options=options)
    results = []
    try:
        driver.get("https://map.kakao.com/")
        time.sleep(1)
        search_box = driver.find_element(By.XPATH, '//*[@id="search.keyword.query"]')
        search_box.clear()
        search_box.send_keys(f"{gu} {keyword}")
        driver.find_element(By.XPATH, '//*[@id="search.keyword.submit"]').send_keys(Keys.ENTER)
        time.sleep(1)

        try:
            no_place = driver.find_element(By.ID, "info.noPlace")
            if "HIDDEN" not in no_place.get_attribute("class"):
                return results
        except NoSuchElementException:
            pass

        try:
            more_btn = driver.find_element(By.ID, "info.search.place.more")
            if "HIDDEN" not in more_btn.get_attribute("class"):
                more_btn.send_keys(Keys.ENTER)
                time.sleep(1)
        except NoSuchElementException:
            pass

        page_index = 1

        while True:
            try:
                next_btn = driver.find_element(By.XPATH, f'//*[@id="info.search.page.next"]')
                if "disabled" in next_btn.get_attribute("class") and page_index == 35:
                    break

                if page_index % 5 == 0:
                    driver.find_element(By.XPATH, f'//*[@id="info.search.page.next"]').send_keys(Keys.ENTER)
                    page_index += 1
                    time.sleep(1)
                else:
                    driver.find_element(By.XPATH, f'//*[@id="info.search.page.no{page_index % 5}"]').send_keys(Keys.ENTER)
                    page_index += 1
                    time.sleep(1)
            except NoSuchElementException:
                break

            time.sleep(1)
            place_lists = driver.find_elements(By.CSS_SELECTOR, r'#info\.search\.place\.list > li')
            print(f"\U0001F50D í˜ì´ì§€ {page_index-1} ì—ì„œ place_lists ê°œìˆ˜: {len(place_lists)}")

            for p in place_lists:
                store_html = p.get_attribute("innerHTML")
                soup = BeautifulSoup(store_html, "html.parser")
                title_tag = soup.select_one('.head_item > .tit_name > .link_name')
                if not title_tag:
                    continue
                name = title_tag.text.strip()
                address = soup.select_one('.info_item > .addr > p').text.strip()
                if gu not in address:
                    continue
                detail_tag = soup.select_one('a.moreview')
                if not detail_tag:
                    continue
                detail_url = detail_tag['href'].strip()
                place_id = detail_url.split("/")[-1]
                lat, lng = get_lat_lng(place_id)
                category_tag = soup.select_one('.subcategory')
                category = category_tag.text.strip() if category_tag else ""
                results.append([keyword, name, category, address, lat, lng, detail_url])

    finally:
        driver.quit()
    return results

def save_results(rows):
    with open(output_file, 'a', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f, delimiter='|')
        writer.writerows(rows)

def already_done(keyword, gu):
    if not os.path.exists(progress_log):
        return False
    with open(progress_log, 'r', encoding='utf-8') as f:
        lines = f.read().splitlines()
    return f"{keyword}|{gu}" in lines

def log_progress(keyword, gu):
    with open(progress_log, 'a', encoding='utf-8') as f:
        f.write(f"{keyword}|{gu}\n")

def run_task(keyword, gu):
    if already_done(keyword, gu):
        print(f"âœ… ìŠ¤í‚µ: {keyword} {gu}")
        return
    print(f"ğŸ” ì§„í–‰ ì¤‘: {keyword} {gu}")
    start_time = time.time()
    rows = crawl_keyword_gu(keyword, gu)
    elapsed = time.time() - start_time
    save_results(rows)
    log_progress(keyword, gu)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {keyword} {gu} ({len(rows)}ê±´) - â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")

def parallel_run():
    processes = []
    combos = [(k, g) for k in search_list for g in seoul_gu_list]

    for i in range(0, len(combos), 4):
        chunk = combos[i:i+4]
        processes = []
        for keyword, gu in chunk:
            p = Process(target=run_task, args=(keyword, gu))
            p.start()
            processes.append(p)
        for p in processes:
            p.join()

if __name__ == "__main__":
    if not os.path.exists(output_file):
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f, delimiter='|')
            writer.writerow(["í‚¤ì›Œë“œ", "ìƒí˜¸ëª…", "ì¹´í…Œê³ ë¦¬", "ë„ë¡œëª…ì£¼ì†Œ", "ìœ„ë„", "ê²½ë„", "ì¹´ì¹´ì˜¤URL"])

    parallel_run()
    print("ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")