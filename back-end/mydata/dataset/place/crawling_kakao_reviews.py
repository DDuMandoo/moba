import pandas as pd
import time
import re
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from multiprocessing import Process, current_process

# í™˜ê²½ ì„¤ì •
base_dir = os.path.dirname(__file__)
INPUT_CSV = os.path.join(base_dir, "seoul_gyeongi_final_place.csv")
OUTPUT_CSV = os.path.join(base_dir, "seoul_gyeongi_final_place_with_rating_TEST.csv")
CHROME_DRIVER_PATH = r"C:/Users/SSAFY/Downloads/chromedriver.exe"

# ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¤ì •
def get_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('lang=ko_KR')
    options.add_argument("user-agent=Mozilla/5.0")
    service = Service(executable_path=CHROME_DRIVER_PATH)
    return webdriver.Chrome(service=service, options=options)

# ë³„ì , ë¦¬ë·°ìˆ˜ ì¶”ì¶œ
def extract_rating_review(driver, url):
    try:
        driver.get(url)
        time.sleep(1.2)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        rating_tag = soup.select_one("span.num_star")
        rating = rating_tag.text.strip() if rating_tag else ""
        review_tag = soup.select_one("span.info_num")
        review = review_tag.text.strip() if review_tag else ""
        return rating, review
    except Exception as e:
        print(f"[{current_process().name}] âŒ {url} - {e}")
        return "", ""

def crawl_rating(start_idx, end_idx):
    print(f"[{current_process().name}] ğŸ”¥ ì‹œì‘! ({start_idx}~{end_idx})")

    df = pd.read_csv(INPUT_CSV, encoding="cp949", low_memory=False).iloc[:1000]
    print(f"[{current_process().name}] ğŸ“Š ë°ì´í„° ê°œìˆ˜: {len(df)}")
    data_chunk = df.iloc[start_idx:end_idx].copy()
    print(f"[{current_process().name}] ğŸ” í¬ë¡¤ë§ ë£¨í”„ ì‹œì‘ - {len(data_chunk)}ê°œ")

    driver = get_driver()
    ratings = []
    reviews = []

    for local_idx, (_, row) in enumerate(data_chunk.iterrows()):
        url = row["ì¹´ì¹´ì˜¤URL"]
        print(f"[{current_process().name}] ğŸŒ {local_idx+1} / {len(data_chunk)} - {url}")
        rating, review = extract_rating_review(driver, url)
        ratings.append(rating)
        reviews.append(review)

        if (local_idx + 1) % 100 == 0:
            print(f"[{current_process().name}] âœ… {local_idx + 1}ê°œ ì™„ë£Œ")

    data_chunk["ë³„ì "] = ratings
    data_chunk["ë¦¬ë·°ìˆ˜"] = reviews

    temp_path = os.path.join(base_dir, f"temp_TEST_{start_idx}_{end_idx}.csv")
    data_chunk.to_csv(temp_path, index=False, encoding="utf-8-sig")
    print(f"[{current_process().name}] ğŸ‰ ì €ì¥ ì™„ë£Œ: {temp_path}")

    driver.quit()

# ë©”ì¸ í•¨ìˆ˜
def parallel_rating_crawl_test():
    df = pd.read_csv(INPUT_CSV, encoding="cp949", low_memory=False).iloc[:1000]
    total = len(df)
    chunk_size = total // 8 + 1

    processes = []
    for i in range(0, total, chunk_size):
        start_idx = i
        end_idx = min(i + chunk_size, total)
        p = Process(target=crawl_rating, args=(start_idx, end_idx), name=f"Proc-{start_idx}-{end_idx}")
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    # ë³‘í•©
    result_dfs = []
    for i in range(0, total, chunk_size):
        temp_path = os.path.join(base_dir, f"temp_TEST_{i}_{min(i+chunk_size, total)}.csv")
        if os.path.exists(temp_path):
            result_dfs.append(pd.read_csv(temp_path))
        else:
            print(f"âš ï¸ {temp_path} ì—†ìŒ!")

    if result_dfs:
        merged_df = pd.concat(result_dfs)
        merged_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
        print(f"âœ… ë³‘í•© ì™„ë£Œ: {OUTPUT_CSV}")

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for i in range(0, total, chunk_size):
            os.remove(os.path.join(base_dir, f"temp_TEST_{i}_{min(i+chunk_size, total)}.csv"))
    else:
        print("âŒ ë³‘í•©í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    parallel_rating_crawl_test()
