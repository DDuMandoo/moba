import pandas as pd
import numpy as np
import os
import re
from gensim.models import KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# âœ… 1. í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ê¸°ì¤€ ì„¤ì •
base_dir = os.path.dirname(os.path.abspath(__file__))

# âœ… 2. FastText ë²¡í„° ë¡œë“œ
model_path = os.path.join(base_dir, "cc.ko.300.kv")
print("ğŸ“¦ Gensim KeyedVectors ë¡œë”© ì¤‘...")
model = KeyedVectors.load(model_path)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# âœ… 3. í˜ë¥´ì†Œë‚˜ ì„¤ëª… ì •ì˜
persona_texts = {
    'A1': "10ëŒ€ í•™ìƒ í•™ì› ë¶„ì‹ íŒ¨ìŠ¤íŠ¸í‘¸ë“œ SNS íŠ¸ë Œë“œ ë˜ë˜ë¬¸í™” ì†Œì•¡ê²°ì œ",
    'A2': "20ëŒ€ ì§ì¥ì¸ ìì·¨ í˜¼ë°¥ ë°°ë‹¬ ì ì‹¬ ì‹ì‚¬ íšŒì‚¬ ê°€ì„±ë¹„",
    'A3': "30~40ëŒ€ ê°€ì¡± ìë…€ í‚¤ì¦ˆì¹´í˜ ì¸í…Œë¦¬ì–´ ì²´í—˜ êµìœ¡ë¹„",
    'A4': "ì¤‘ì¥ë…„ ê±´ê°• í•œì‹ ì—¬í–‰ ê³µì—° ì˜¤í”„ë¼ì¸ ë¸Œëœë“œ",
    'B1': "ìš´ë™ í—¬ìŠ¤ ë‹¨ë°±ì§ˆ ê³ ê¸° ìƒëŸ¬ë“œ ë°€í‚¤íŠ¸",
    'B2': "í•´ì‚°ë¬¼ íšŒ ì¡°ê°œ ì¼ì‹ ëìŠ¤í„° ìˆ˜ì‚°ë¬¼ ê³ ê¸‰",
    'B3': "ì±„ì‹ ì›°ë¹™ ë¹„ê±´ ìœ ê¸°ë† ì €ì¹¼ë¡œë¦¬ ì¹œí™˜ê²½",
    'B4': "ë¶„ì‹ ë„ì‹œë½ íŒ¨ìŠ¤íŠ¸í‘¸ë“œ í¸ì˜ì  ê°„í¸ì‹ ì•¼ì‹",
    'B5': "ì„¸ê³„ìŒì‹ ë©•ì‹œì¹¸ íƒœêµ­ ì¸ë„ í“¨ì „ SNS ì¸ì¦",
    'C1': "ê°€ì„±ë¹„ í• ì¸ ì¿ í° ì¤‘ê³  ì´ì›” ì ë¦½",
    'C2': "í”„ëœì°¨ì´ì¦ˆ ëŒ€ì¤‘ ë¸Œëœë“œ í‰ê·  ê°€ì‹¬ë¹„",
    'C3': "í”„ë¦¬ë¯¸ì—„ ëª…í’ˆ ê³ ê¸‰ ì™¸ì‹ í˜¸í…” ìŠ¤íŒŒ ê³¨í”„",
    'C4': "SNS í•«í”Œ íŠ¸ë Œë”” íŒì—… ì „ì‹œ í•œì •íŒ êµ¿ì¦ˆ",
    'C5': "í™ˆì¹´í˜ í™ˆì¿¡ OTT ë°€í‚¤íŠ¸ ì§‘ì½• ë°°ë‹¬ ìŠ¤íŠ¸ë¦¬ë°",
    'D1': "ë ˆì € ìº í•‘ ë“±ì‚° ë°”ë‹¤ ê³„ê³¡ ëª¨í—˜ ì°¨ë°•",
    'D2': "ì „ì‹œ ë¯¸ìˆ ê´€ ê³µì—° ì˜¤í˜ë¼ ì—°ê·¹ ë¬¸í™”ì˜ˆìˆ ",
    'D3': "ê²Œì„ ë…¸ë˜ë°© í´ëŸ½ ì˜¤ë½ VR ìœ í¥",
    'D4': "ê³µë°© ì›ë°ì´í´ë˜ìŠ¤ ìê²©ì¦ ìê¸°ê³„ë°œ ì·¨ë¯¸ í•™ì›",
    'E1': "1ì¸ ê°€êµ¬ í˜¼ë°¥ í˜¼ìˆ  ì†Œí¬ì¥ ì†Œí˜•ê°€êµ¬",
    'E2': "ìë…€ í‚¤ì¦ˆì¹´í˜ í•™ì› ìœ ì•„ìš©í’ˆ ê°€ì¡± ë†€ì´ê³µì›",
    'E3': "ë°˜ë ¤ë™ë¬¼ ê°•ì•„ì§€ ê³ ì–‘ì´ í« ìš©í’ˆ ë¯¸ìš©"
}

def clean_and_tokenize(text):
    text = re.sub(r"[^\w\s]", " ", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    tokens = text.split()
    return tokens

# âœ… 4. ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def sentence_vector(sentence, model):
    tokens = clean_and_tokenize(sentence)
    vectors = [model[word] for word in tokens if word in model]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)

# âœ… 5. í˜ë¥´ì†Œë‚˜ ë²¡í„°í™”
persona_vecs = {k: sentence_vector(v, model) for k, v in persona_texts.items()}

# âœ… 6. ì†Œë¶„ë¥˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
csv_path = os.path.join(base_dir, "categories.csv")
df = pd.read_csv(csv_path, encoding="cp949")  # or "utf-8" if needed

# âœ… 7. ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤í•‘
results = []

print("ğŸ” ì†Œë¶„ë¥˜ë³„ í˜ë¥´ì†Œë‚˜ ë§¤í•‘ ì¤‘...")
for _, row in tqdm(df.iterrows(), total=len(df)):
    text = f"{row['ì„¸ë¶€ë¶„ë¥˜']} {row['ì—°ê´€ë„ì¹´í…Œê³ ë¦¬']} {row['ì¹´í…Œê³ ë¦¬']}"
    vec = sentence_vector(text, model)
    sims = {k: cosine_similarity([vec], [v])[0][0] for k, v in persona_vecs.items()}
    top5 = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:5]

    results.append({
        'ì„¸ë¶€ë¶„ë¥˜': row['ì„¸ë¶€ë¶„ë¥˜'],
        'ì—°ê´€ë„ì¹´í…Œê³ ë¦¬': row['ì—°ê´€ë„ì¹´í…Œê³ ë¦¬'],
        'ì¹´í…Œê³ ë¦¬': row['ì¹´í…Œê³ ë¦¬'],
        'Top1': top5[0][0], 'Sim1': round(top5[0][1], 3),
        'Top2': top5[1][0], 'Sim2': round(top5[1][1], 3),
        'Top3': top5[2][0], 'Sim3': round(top5[2][1], 3),
        'Top4': top5[3][0], 'Sim4': round(top5[3][1], 3),
        'Top5': top5[4][0], 'Sim5': round(top5[4][1], 3),
    })

# âœ… 8. ê²°ê³¼ ì €ì¥
output_path = os.path.join(base_dir, "persona_mapped_fasttext.csv")
pd.DataFrame(results).to_csv(output_path, index=False, encoding="utf-8")
print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
